#change permission of pem file first
chmod 400 abc.pem

#to access server via ssh
ssh -i abc.pem user@ec2-8757859vftsgjvbvbsv-ap-south-1.compute.amazonaws.com

#kill application
ec2-8757859vftsgjvbvbsv-ap-south-1.compute.amazonaws.com:8088
#Go to individual application id and kill

#spark ui port
18080

#see list of yarn applications
yarn application -list

#see list of all applications
sudo initctl list

#s3 bucket access / see
aws s3 ls s3://<bucket_name>/folder

#s3 bucket file copy / send
aws s3 cp abc.csv s3://<bucket_name>/folder



###############Dealing with s3 files in pandas##########
import pandas as pd
import boto3
from io import StringIO

bucket = "yourbucket"
file_name = "your_file.csv"

s3 = boto3.client('s3') 
# 's3' is a key word. create connection to S3 using default config and all buckets within S3

obj = s3.get_object(Bucket= bucket, Key= file_name) 
# get object and file (key) from bucket
initial_df = pd.read_csv(obj['Body']) # 'Body' is a key word

csv_buffer = StringIO()
initial_df.to_csv(csv_buffer)
s3_resource = boto3.resource('s3')
resp = s3_resource.Object(bucket, 'folder/sub_folder/initial_df_file.csv').put(Body=csv_buffer.getvalue())
if resp['ResponseMetadata']['HTTPStatusCode'] == 200:
    print("All ok")
else:
    print("file not inserted in s3")

#Reference: https://stackoverflow.com/questions/43355074/read-a-csv-file-from-aws-s3-using-boto-and-pandas
#########################################################

